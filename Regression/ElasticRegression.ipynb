{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ElasticRegression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4Qjb1ijDD8S"
      },
      "source": [
        "##We are creating a class with elastic regression. Elastic regression is the combination of lasso and ridge. Mostly the logic behind the Elastic Net Loss function is same in this we add two more terms in loss function , one for ridge and one for lasso. When we use both the functions it is called as an elastic net loss function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_KJ0dCMk7H_"
      },
      "source": [
        "#Almost all the comments are same as in question 2\n",
        "#Declaring the Elastic Gradient descent class\n",
        "class ElasticGradientDescendent:\n",
        "\n",
        "  #Declaring the values of gama , lasso and ridge and setting their values as zero by default\n",
        "  def __init__(gama = 0 , lasso = 0 , ridge = 0):\n",
        "    self.lasso = lasso\n",
        "    self.ridge = ridge\n",
        "    self.gama = gama\n",
        "\n",
        "\n",
        "\n",
        "  #All the values are same as in above Q2\n",
        "  def fit(self, X, y, x0, alpha, h, tolerance, maxIterations):\n",
        "        self.n = X.shape[0]\n",
        "        self.d = X.shape[1]\n",
        "        self.h = h\n",
        "        self.alpha = alpha\n",
        "        self.x0 = x0\n",
        "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
        "        \n",
        "       \n",
        "        self.outputs = y\n",
        "        \n",
        "       \n",
        "        X = self.data\n",
        "        #Two more terms are added in the loss function one for lasso and one for ridge\n",
        "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y) + self.lasso * np.linalg.norm(beta, ord=1) + self.ridge * np.linalg.norm(beta, ord=2)\n",
        "        self.beta = self.gradientDescent(L, self.initialGuess, self.alpha, self.h, self.decay_rate, tolerance, maxIterations)\n",
        "                \n",
        "       \n",
        "\n",
        "\n",
        "  def predict(self, X):\n",
        "\n",
        "        yPredicted = np.empty([X.shape[0],1])\n",
        "        \n",
        "        \n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        \n",
        "        \n",
        "        for row in range(X.shape[0]):\n",
        "            yPredicted[row] = self.beta @ X[row,]\n",
        "            \n",
        "        return yPredicted\n",
        "  def gradientDescendent(self, f, X0, h,  alpha, gama,  tolerance, maxIterations):\n",
        "  \n",
        "    x = X0\n",
        "    n = len(x)\n",
        "    vt = np.zeros(n)\n",
        "\n",
        "\n",
        "    for counter in range(maxIterations):\n",
        "\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "              print(counter)\n",
        "            \n",
        "              if np.linalg.norm(gradient) < tolerance:\n",
        "                  print('Gradient descent took', counter, 'iterations to converge')\n",
        "                  print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "                \n",
        "                  return x\n",
        "\n",
        "            \n",
        "              elif counter == maxIterations-1:\n",
        "                  print(\"Gradient descent failed\")\n",
        "                  print('The gradient is', gradient)\n",
        "             \n",
        "                  return x\n",
        "            \n",
        "              vt = alpha*gradient + vt*gama\n",
        "              x -= vt\n",
        "\n",
        "  def computeGradient(self,f,x,h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}