{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearRegression_Using_OdinayLeastSquarer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJz-Ks2GNy7D"
      },
      "source": [
        "A gradient descent function is created in Ordinary least square class, this function helps to find the gradient descent by computing the gradient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R0SG_zT6qjr"
      },
      "source": [
        "# Importing all the important modules\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.preprocessing import scale\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas \n",
        "import time\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WrTGxFP6szM"
      },
      "source": [
        "#class declaration \n",
        "class OrdinaryLeastSquare:\n",
        "\n",
        "  #declaring gama which will be used to calculate SGD with momentum\n",
        "  #By Default set to zero to perform SGD without Momentum\n",
        "  def __init__(self, gama=0):\n",
        "    self.gama = gama\n",
        "\n",
        "  \n",
        "\n",
        "  #Declaring the fit class to fit the data in the model and train it\n",
        "  #It has several varaiables, X the training data, Y the labels , alpha the learning rate , h used to computer gradient by approximation,\n",
        "  #Tolerance is the value till which the gradient will go close , maxIterations is the max value till which gradientdescend try to find X value\n",
        "  def fit(self, X, y, x0, alpha, h, tolerance, maxIterations):\n",
        "        self.n = X.shape[0]\n",
        "        self.d = X.shape[1]\n",
        "        self.h = h\n",
        "        self.alpha = alpha\n",
        "        self.x0 = x0\n",
        "        #Adding ones column in the data as in Y = mx + c , there is one more feature on right side compared to left\n",
        "        self.data = np.hstack((np.ones([self.n, 1]), X))\n",
        "        \n",
        "       \n",
        "        self.outputs = y\n",
        "        \n",
        "       \n",
        "        X = self.data\n",
        "        #Using the Loss function\n",
        "        #Trying to find the derivative\n",
        "        L = lambda beta: ((X @ beta).T - y.T) @ (X @ beta - y) \n",
        "        self.beta = self.gradientDescendent(L, self.x0,   self.h, self.alpha,  self.gama, tolerance, maxIterations)\n",
        "\n",
        "\n",
        "  #Declaring the predict class\n",
        "  def predict(self, X):\n",
        "\n",
        "        yPredicted = np.empty([X.shape[0],1])\n",
        "        \n",
        "        ##Adding ones column in the data as in Y = mx + c , there is one more feature on right side compared to left\n",
        "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
        "        \n",
        "        # Applying function f to each datapoint\n",
        "        for row in range(X.shape[0]):\n",
        "            yPredicted[row] = self.beta @ X[row,]\n",
        "            \n",
        "        return yPredicted\n",
        "\n",
        "  #Declaring the function to calculate the gradient descendt\n",
        "  #All the values mean same as above\n",
        "  def gradientDescendent(self, f, X0, h,  alpha, gama,  tolerance, maxIterations):\n",
        "  \n",
        "    x = X0\n",
        "    n = len(x)\n",
        "    vt = np.zeros(n)\n",
        "\n",
        "\n",
        "    for counter in range(maxIterations):\n",
        "\n",
        "              gradient = self.computeGradient(f, x, h)\n",
        "              #print(counter)\n",
        "            \n",
        "              if np.linalg.norm(gradient) < tolerance:\n",
        "                  print('Gradient descent took', counter, 'iterations to converge')\n",
        "                  print('The norm of the gradient is', np.linalg.norm(gradient))\n",
        "                \n",
        "                  return x\n",
        "\n",
        "            \n",
        "              elif counter == maxIterations-1:\n",
        "                  print(\"Gradient descent failed\")\n",
        "                  print('The gradient is', gradient)\n",
        "             \n",
        "                  return x\n",
        "            \n",
        "              vt = alpha*gradient + vt*gama\n",
        "              x -= vt\n",
        "\n",
        "  #Calculating the gradient by approximation \n",
        "  def computeGradient(self,f,x,h):\n",
        "        n = len(x)\n",
        "        gradient = np.zeros(n)\n",
        "\n",
        "        for counter in range(n):\n",
        "            xUp = x.copy()\n",
        "            xUp[counter] += h\n",
        "            gradient[counter] = (f(xUp) - f(x))/h\n",
        "\n",
        "        return gradient\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ix0BEp3UZoH"
      },
      "source": [
        "#Preprocess the data before training or testing\n",
        "\n",
        "#importing module to deal with CSV\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "#Reading the csv file from drive\n",
        "data = pd.read_csv('/content/drive/My Drive/Mount_Pleasant_Real_Estate_Data.csv', sep=',')\n",
        "\n",
        "#Remove all the data with Null/NAN values\n",
        "data = data.dropna(axis=0)\n",
        "\n",
        "\n",
        "#Removing the unecesasry columns\n",
        "drop_features = [\"ID\",\"Baths - Full\", \"Baths - Half\", \"Misc Exterior\",\n",
        "                 \"Amenities\", \"Fireplace?\", \"Subdivision\", \"House Style\"]\n",
        "data.drop(drop_features, axis=1, inplace=True)\n",
        "\n",
        "#data.head()\n",
        "\n",
        "#Replacing the comma and $ with ''\n",
        "data['List Price'] = data['List Price'].str.replace(',', '').str.replace('$', '').astype(float)\n",
        "\n",
        "\n",
        "#Mapping the yes/no to binary 1/0\n",
        "features = ['Duplex?', 'New Owned?', 'Has Pool?', 'Has Dock?', 'Fenced Yard', 'Screened Porch?', 'Golf Course?']\n",
        "for feature in features:\n",
        "  data[feature] = data[feature].map(dict(Yes=1, No=0))\n",
        "\n",
        "\n",
        "#taking the mean of the data to normalize\n",
        "data_mean = data.mean()\n",
        "\n",
        "data_max = data.max()\n",
        "data_min = data.min()\n",
        "\n",
        "\n",
        "data = (data-data_mean)/(data_max-data_min)\n",
        "\n",
        "\n",
        "data = data.to_numpy()\n",
        "\n",
        "\n",
        "#taking trainX and trainY to train the model\n",
        "trainX = np.array(data[:,1:], dtype=float)\n",
        "trainy = np.array(data[:,0], dtype=float)\n",
        "\n",
        "#Decalring an initial value a\n",
        "initial_value = (np.random.uniform(size=trainX.shape[1]+1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx1mZD6cU8WO",
        "outputId": "e8066f86-28d0-4cee-b458-24d3cdba3946",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#SGD without Momentum , as the gama value is set to zero\n",
        "model = OrdinaryLeastSquare(gama = 0)\n",
        "\n",
        "#Starting the time counter \n",
        "time_start = time.time()\n",
        "#training the model\n",
        "model.fit(trainX, trainy, initial_value,  h = 0.001, alpha = 0.001,  tolerance = 0.0001, maxIterations = 100000)\n",
        "\n",
        "#Ending the time counter\n",
        "time_end = time.time()\n",
        "print('Time : ', (time_end-time_start), 'seconds')\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "parameters = model.beta\n",
        "\n",
        "#Calulating the different values for analysis\n",
        "print('The r**2 score is', r2_score(trainy, predictions))\n",
        "print('The mean squared error is', mean_squared_error(trainy, predictions))\n",
        "print('The mean absolute error is', mean_absolute_error(trainy, predictions),'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient descent took 5743 iterations to converge\n",
            "The norm of the gradient is 9.990350506882552e-05\n",
            "Time :  2.0678915977478027 seconds\n",
            "The r**2 score is 0.9062664928934926\n",
            "The mean squared error is 0.0025743828942834685\n",
            "The mean absolute error is 0.03573018436428183 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f4SrPPfXYg4",
        "outputId": "a4ab7445-a45a-4bbb-92fb-9af60c6dc4f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "#SGD with momentum , as the gama value is not set zero it is set to 0.9 \n",
        "model = OrdinaryLeastSquare( gama=0.9)\n",
        "initial_value = (np.random.uniform(size=trainX.shape[1]+1))\n",
        "\n",
        "\n",
        "time_start = time.time()\n",
        "\n",
        "model.fit(trainX, trainy, initial_value, alpha = 0.001, h = 0.001, tolerance = 0.0001, maxIterations = 100000)\n",
        "\n",
        "time_end = time.time()\n",
        "print('Time : ', (time_end-time_start), 'seconds')\n",
        "predictions = model.predict(trainX)\n",
        "\n",
        "parameters = model.beta\n",
        "\n",
        "#Calulating the values for analysis\n",
        "print('The r**2 score is', r2_score(trainy, predictions))\n",
        "print('The mean squared error is', mean_squared_error(trainy, predictions))\n",
        "print('The mean absolute error is', mean_absolute_error(trainy, predictions),'\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gradient descent took 467 iterations to converge\n",
            "The norm of the gradient is 9.948595303788544e-05\n",
            "Time :  0.30265259742736816 seconds\n",
            "The r**2 score is 0.9062664927453994\n",
            "The mean squared error is 0.0025743828983508356\n",
            "The mean absolute error is 0.035730184883892624 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}